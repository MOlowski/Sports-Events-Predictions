{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df04cc06-45c2-4d66-a2b5-58d6154d6583",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_data(endpoint, params):\n",
    "    \n",
    "    URL = \"https://v3.football.api-sports.io/\"\n",
    "    headers = {\n",
    "\t'x-rapidapi-host': \"v3.football.api-sports.io\",\n",
    "    'x-rapidapi-key': \"fb2140228973d644db847895c454c22b\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        URL+endpoint,\n",
    "        headers = headers,\n",
    "        params = params\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "            \n",
    "        remaining = response.headers.get(\"x-ratelimit-requests-remaining\")\n",
    "        data = response.json()\n",
    "        print(f\"requests before reaching limit {remaining}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}, {response.text}\")\n",
    "\n",
    "    return data, remaining                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94a0a330-d214-47f0-b9e2-bd1aedfd9e04",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def encode_data(data_dict, parent_key = '', sep= '_'):\n",
    "    encoded = []\n",
    "    for key, val in data_dict.items():\n",
    "        new_key = f'{parent_key}{sep}{key}' if parent_key else key\n",
    "        if isinstance(val, dict):\n",
    "            encoded.extend(encode_data(val, new_key, sep=sep).items())\n",
    "        elif isinstance(val, list):\n",
    "            if val:\n",
    "                if all(isinstance(i, dict) for i in val):\n",
    "                    for k, v in enumerate(val):\n",
    "                        v_key = f'{new_key}{sep}{k}'\n",
    "                        encoded.extend(encode_data(v, v_key, sep=sep).items())\n",
    "                else:\n",
    "                    encoded.append((new_key, val))\n",
    "            else:\n",
    "                encoded.append((new_key, []))\n",
    "        else:\n",
    "            encoded.append((new_key, val))\n",
    "    return dict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc3226-81ed-4ee3-aa33-384b6be1d126",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def encode_fix_stats(data, fixture_id):\n",
    "    encoded = []\n",
    "    encoded.append(('fixture_id', fixture_id))\n",
    "    for key, val in data.items():\n",
    "        if key =='team':\n",
    "            encoded.append((key+'_id', val['id']))\n",
    "        else:\n",
    "            for el in val:\n",
    "                encoded.append((el['type'].lower().replace(' ', '_').replace('%', 'percentage'), el['value']))\n",
    "    return dict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a41b6c46-cefc-4821-9f20-f7e5cbeb42f9",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_to_sql(table_name, df, db_params, conflict_columns):\n",
    "    conn = None\n",
    "    cur = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        #insert data into tables\n",
    "        if len(conflict_columns) == 0:\n",
    "            insert_query = \"\"\"\n",
    "                INSERT INTO {} ({})\n",
    "                VALUES ({})\n",
    "            \"\"\".format(table_name, ','.join(df.columns), ','.join(['%s']*len(df.columns)))\n",
    "        else:\n",
    "            insert_query = \"\"\"\n",
    "                INSERT INTO {} ({})\n",
    "                VALUES ({})\n",
    "                ON CONFLICT ({}) DO NOTHING\n",
    "            \"\"\".format(table_name, ','.join(df.columns), ','.join(['%s']*len(df.columns)), ','.join(conflict_columns))\n",
    "        if len(df) > 0:\n",
    "            last_row = df.iloc[-1]\n",
    "        cur.executemany(insert_query, df.values.tolist())\n",
    "        print(f'table {table_name} updated')\n",
    "        \n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if last_row is not None:\n",
    "            print(f\"Last row loaded before the error occurred: {last_row}\")\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            # Close the cursor and connection\n",
    "            cur.close()\n",
    "        if cur is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb05e97-a7e2-4067-8c5e-89c314a2b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matches(db_params):\n",
    "    query = '''\n",
    "    SELECT fixture_id\n",
    "    FROM fixtures\n",
    "    WHERE fixture_status_short = 'FT' or fixture_status_short = 'WO' or fixture_status_short = 'AET' or fixture_status_short = 'PEN' or fixture_status_short = 'CANC'\n",
    "    '''\n",
    "    res = []\n",
    "    conn = None\n",
    "    cur = None\n",
    "    \n",
    "    try:\n",
    "        conn = pdsycopg2.connect(db_params)\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        cur.execute(query)\n",
    "        res = [row[0] for row in cur.fetchall()]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error {e} occured')\n",
    "\n",
    "    finally:\n",
    "        if cur is not None:\n",
    "            cur.close()\n",
    "        if conn is not None:\n",
    "            conn.close()\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e6a0ec-5dd3-4c33-b815-08bbb8cb7177",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requests before reaching limit 97\n",
      "97\n",
      "requests before reaching limit 96\n",
      "96\n",
      "requests before reaching limit 95\n",
      "95\n",
      "requests before reaching limit 94\n",
      "94\n",
      "requests before reaching limit 92\n",
      "92\n",
      "requests before reaching limit 91\n",
      "91\n",
      "requests before reaching limit 90\n",
      "90\n",
      "requests before reaching limit 89\n",
      "89\n",
      "requests before reaching limit 96\n",
      "96\n",
      "requests before reaching limit 95\n",
      "95\n",
      "requests before reaching limit 94\n",
      "94\n",
      "requests before reaching limit 93\n",
      "93\n",
      "requests before reaching limit 84\n",
      "84\n",
      "requests before reaching limit 83\n",
      "83\n",
      "requests before reaching limit 82\n",
      "82\n",
      "requests before reaching limit 81\n",
      "81\n",
      "requests before reaching limit 88\n",
      "88\n",
      "requests before reaching limit 87\n",
      "87\n",
      "requests before reaching limit 86\n",
      "86\n",
      "requests before reaching limit 85\n",
      "85\n",
      "requests before reaching limit 76\n",
      "76\n",
      "requests before reaching limit 75\n",
      "75\n",
      "requests before reaching limit 74\n",
      "74\n",
      "requests before reaching limit 73\n",
      "73\n",
      "requests before reaching limit 80\n",
      "80\n",
      "requests before reaching limit 79\n",
      "79\n",
      "requests before reaching limit 78\n",
      "78\n",
      "requests before reaching limit 77\n",
      "77\n",
      "requests before reaching limit 68\n",
      "68\n",
      "requests before reaching limit 67\n",
      "67\n",
      "requests before reaching limit 66\n",
      "66\n",
      "requests before reaching limit 65\n",
      "65\n",
      "requests before reaching limit 72\n",
      "72\n",
      "requests before reaching limit 71\n",
      "71\n",
      "requests before reaching limit 70\n",
      "70\n",
      "requests before reaching limit 69\n",
      "69\n",
      "requests before reaching limit 60\n",
      "60\n",
      "requests before reaching limit 59\n",
      "59\n",
      "requests before reaching limit 58\n",
      "58\n",
      "requests before reaching limit 64\n",
      "64\n",
      "requests before reaching limit 63\n",
      "63\n",
      "requests before reaching limit 62\n",
      "62\n",
      "requests before reaching limit 61\n",
      "61\n",
      "requests before reaching limit 60\n",
      "60\n",
      "requests before reaching limit 52\n",
      "52\n",
      "requests before reaching limit 51\n",
      "51\n",
      "requests before reaching limit 50\n",
      "50\n",
      "requests before reaching limit 57\n",
      "57\n",
      "requests before reaching limit 56\n",
      "56\n",
      "requests before reaching limit 55\n",
      "55\n",
      "requests before reaching limit 54\n",
      "54\n",
      "requests before reaching limit 53\n",
      "53\n",
      "requests before reaching limit 44\n",
      "44\n",
      "requests before reaching limit 43\n",
      "43\n",
      "requests before reaching limit 42\n",
      "42\n",
      "requests before reaching limit 49\n",
      "49\n",
      "requests before reaching limit 48\n",
      "48\n",
      "requests before reaching limit 47\n",
      "47\n",
      "requests before reaching limit 46\n",
      "46\n",
      "requests before reaching limit 45\n",
      "45\n",
      "requests before reaching limit 36\n",
      "36\n",
      "requests before reaching limit 35\n",
      "35\n",
      "requests before reaching limit 34\n",
      "34\n",
      "requests before reaching limit 41\n",
      "41\n",
      "requests before reaching limit 40\n",
      "40\n",
      "requests before reaching limit 39\n",
      "39\n",
      "requests before reaching limit 38\n",
      "38\n",
      "requests before reaching limit 37\n",
      "37\n",
      "requests before reaching limit 28\n",
      "28\n",
      "requests before reaching limit 27\n",
      "27\n",
      "requests before reaching limit 26\n",
      "26\n",
      "requests before reaching limit 33\n",
      "33\n",
      "requests before reaching limit 32\n",
      "32\n",
      "requests before reaching limit 31\n",
      "31\n",
      "requests before reaching limit 30\n",
      "30\n",
      "requests before reaching limit 29\n",
      "29\n",
      "requests before reaching limit 20\n",
      "20\n",
      "requests before reaching limit 19\n",
      "19\n",
      "requests before reaching limit 25\n",
      "25\n",
      "requests before reaching limit 24\n",
      "24\n",
      "requests before reaching limit 23\n",
      "23\n",
      "requests before reaching limit 22\n",
      "22\n",
      "requests before reaching limit 21\n",
      "21\n",
      "requests before reaching limit 20\n",
      "20\n",
      "requests before reaching limit 12\n",
      "12\n",
      "requests before reaching limit 11\n",
      "11\n",
      "requests before reaching limit 18\n",
      "18\n",
      "requests before reaching limit 17\n",
      "17\n",
      "requests before reaching limit 16\n",
      "16\n",
      "requests before reaching limit 15\n",
      "15\n",
      "requests before reaching limit 14\n",
      "14\n",
      "requests before reaching limit 13\n",
      "13\n",
      "requests before reaching limit 4\n",
      "4\n",
      "requests before reaching limit 3\n",
      "3\n",
      "requests before reaching limit 10\n",
      "10\n",
      "requests before reaching limit 9\n",
      "9\n",
      "requests before reaching limit 8\n",
      "8\n",
      "requests before reaching limit 7\n",
      "7\n",
      "requests before reaching limit 6\n",
      "6\n",
      "requests before reaching limit 5\n",
      "5\n",
      "requests before reaching limit -4\n",
      "-4\n",
      "Error: BŁĄD:  błąd składni w lub blisko \"-\"\n",
      "LINE 2: ...ge_away,goals_for_average_total,goals_for_minute_0-15_total,...\n",
      "                                                             ^\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'teams_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 119\u001b[0m\n\u001b[0;32m    116\u001b[0m     data_to_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_stats\u001b[39m\u001b[38;5;124m'\u001b[39m, team_stats_df, db_params, conflict_col)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(total_teams_data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 119\u001b[0m     teams_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mteams_df\u001b[49m)\n\u001b[0;32m    120\u001b[0m     teams_df \u001b[38;5;241m=\u001b[39m teams_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_country\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_logo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_national\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenue_capacity\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvenue_surface\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mrename(columns \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_national\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnational\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m    121\u001b[0m     conflict_col \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'teams_df' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# get current and european seasons files from bucket\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sport-storage'\n",
    "object_key1 = 'current.csv'\n",
    "object_key2 = 'european_seasons.csv'\n",
    "object_key3 = 'teams.json'\n",
    "\n",
    "response1 = s3.get_object(Bucket = bucket_name, Key = object_key1)\n",
    "response2 = s3.get_object(Bucket = bucket_name, Key = object_key2)\n",
    "response3 = s3.get_object(Bucket = bucket_name, Key = object_key3)\n",
    "\n",
    "current = pd.read_csv(io.BytesIO(response1['Body'].read()))\n",
    "european_seasons = pd.read_csv(io.BytesIO(response2['Body'].read()))\n",
    "\n",
    "# insert parameters to be found\n",
    "league = current['league_id'][0]\n",
    "year = current['year'][0]\n",
    "to_find = current['type'][0]\n",
    "\n",
    "# get current index\n",
    "index = np.where((european_seasons['league_id']==league)&(european_seasons['year']==year))[0][0]\n",
    "\n",
    "remaining = 100\n",
    "total_team_stats_data = []\n",
    "total_teams_data = []\n",
    "total_fixtures_data = []\n",
    "\n",
    "if index == len(european_seasons)-1:\n",
    "    done = True\n",
    "else:\n",
    "    done = False\n",
    "\n",
    "# if script ended on collecting team stats get rest of teams\n",
    "if to_find == 'teams/statistics':\n",
    "    teams_list = pickle.loads(response3['Body'].read())\n",
    "else:\n",
    "    teams_list = []\n",
    "\n",
    "# if limit of requests wasnt reached and there is still something to collect enter loop\n",
    "while (done==False)&(remaining > 0):\n",
    "\n",
    "    # get data from API\n",
    "    if to_find == 'teams': \n",
    "        params = {'league': league,\n",
    "                  'season': year\n",
    "                 }\n",
    "    elif to_find == 'teams/statistics':\n",
    "        team = teams_list[0]\n",
    "        params = {'league': league,\n",
    "                  'season': year,\n",
    "                  'team': team\n",
    "                 }\n",
    "    else:\n",
    "        params = {'league': league,\n",
    "                  'season': year}\n",
    "    \n",
    "    endpoint = to_find\n",
    "    data, remaining_req = get_data(endpoint, params)\n",
    "    \n",
    "    # find all teams played in each league in each season\n",
    "    if endpoint == 'teams':\n",
    "        total_teams_data.extend(encode_data(team) for team in data['response'])\n",
    "        to_find = 'fixtures'\n",
    "\n",
    "    # find all fixtures played in each league in each season\n",
    "    elif endpoint == 'fixtures':\n",
    "        total_fixtures_data.extend(encode_data(fix) for fix in data['response'])\n",
    "        \n",
    "        # if last endpoint is fixtures now find stats for every team in that season\n",
    "        to_find = 'teams/statistics'\n",
    "        teams_list = [row['team']['id'] for row in data['response']]\n",
    "\n",
    "    # find all stats for each teams played in each league in each season\n",
    "    else:\n",
    "        total_team_stats_data.append(encode_data(data['response']))\n",
    "        \n",
    "        # drop team that data was already collected for\n",
    "        if len(teams_list) > 0:\n",
    "            teams_list.pop(0)\n",
    "\n",
    "        # if team stats were collected for every team in current season move to next season\n",
    "        elif (len(teams_list) == 0) & (index < len(european_seasons)-1):\n",
    "            index += 1\n",
    "            league = european_seasons.loc[index]['league_id']\n",
    "            year = european_seasons.loc[index]['year']\n",
    "            to_find = 'teams'\n",
    "\n",
    "        # if data for all seasons were collected quit loop\n",
    "        else:\n",
    "            done = True\n",
    "            to_find = 'fixtures/statistics'\n",
    "        \n",
    "    remaining = int(remaining_req)\n",
    "    print(remaining)\n",
    "    # sleep cause there can be done only 10 requests per minute\n",
    "    time.sleep(7)\n",
    "    \n",
    "    # saving to sql database\n",
    "    # db parameters\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'preds',\n",
    "    'user': 'postgres',\n",
    "    'password': 'pass',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "\n",
    "if len(total_team_stats_data) > 0:\n",
    "    team_stats_df = pd.DataFrame(total_team_stats_data)\n",
    "    team_stats_df = team_stats_df.drop(columns = {\n",
    "        'league_name', \n",
    "        'league_country', \n",
    "        'league_logo', \n",
    "        'league_flag', \n",
    "        'team_name', \n",
    "        'team_logo',\n",
    "        'lineups'})\n",
    "    team_stats_df = team_stats_df.fillna(0)\n",
    "    conflict_col = []\n",
    "    team_stats_df.columns = team_stats_df.columns.str.replace('-','_')\n",
    "    data_to_sql('team_stats', team_stats_df, db_params, conflict_col)\n",
    "                \n",
    "if len(total_teams_data) > 0:\n",
    "    teams_df = pd.DataFrame(total_teams_data)\n",
    "    teams_df = teams_df[['team_id', \n",
    "                         'team_name', \n",
    "                         'team_country', \n",
    "                         'team_logo', \n",
    "                         'team_national', \n",
    "                         'venue_capacity', \n",
    "                         'venue_surface']].rename(columns = {'team_national': 'national'})\n",
    "    conflict_col = ['team_id']\n",
    "    data_to_sql('teams', teams_df, db_params, conflict_col)\n",
    "\n",
    "if len(total_fixtures_data) > 0:\n",
    "    fixtures_df = pd.DataFrame(total_fixtures_data)\n",
    "    fixtures_df = fixtures_df.drop(columns = {\n",
    "        'fixture_timezone',\n",
    "        'fixture_timestamp', \n",
    "        'fixture_periods_first', \n",
    "        'fixture_periods_second',\n",
    "        'league_name',\n",
    "        'league_country',\n",
    "        'league_logo',\n",
    "        'league_flag',\n",
    "        'fixture_venue_city',\n",
    "        'fixture_venue_name',\n",
    "        'teams_away_logo',\n",
    "        'teams_away_name',\n",
    "        'teams_home_logo',\n",
    "        'teams_home_name'})\n",
    "conflict_col = ['fixture_id']\n",
    "data_to_sql('fixtures', fixtures_df, db_params, conflict_col)\n",
    "\n",
    "    #\n",
    "    json_data = json.dumps(teams_list)\n",
    "    s3.put_object(Bucket = bucket_name, Key = object_key3, Body=json_data)\n",
    "\n",
    "if not done:\n",
    "    print(f'{len(european_seasons)-index-1} seasons left')\n",
    "else:\n",
    "    print('Teams and their statistics were collected for every season played in Europe')\n",
    "\n",
    "# if all teams, teams statistics and fixtures were collected get fixtures stats\n",
    "if to_find == 'fixtures/statistics':\n",
    "    total_fix_stats_data = []\n",
    "    endpoint = to_find\n",
    "    matches_key = 'matches.csv'\n",
    "    matches_response = s3.get_object(Bucket = bucket_name, Key = matches_key)\n",
    "    matches = pickle.loads(matches_response['Body'].read())\n",
    "\n",
    "    # if there are no fixtures ids in list get fixtures ids from fixtures table\n",
    "    if len(matches) == 0:\n",
    "        matches = get_matches()\n",
    "    # if all fixtures stats were collected list contains only 'done'\n",
    "    if matches[0] == 'done':\n",
    "        print('matches statistics collected')\n",
    "    else:\n",
    "        while (len(matches) > 0) & (remaining > 0):\n",
    "            fixture = matches[0]\n",
    "            params = {'fixture': fixture}\n",
    "            df, remaining = get_data(endpoint, params)\n",
    "            total_fix_stats_data.extend(encode_fix_stats(row, fixture) for row in df['response'])\n",
    "            matches.pop(0)\n",
    "        if len(total_fix_stats_data) > 0:\n",
    "            fixture_stats_df = pd.DataFrame(total_fix_stats_data)\n",
    "            conflict_col = ['fixture_id', 'team_id']\n",
    "            data_to_sql('fixture_statistics', fixture_stats_df, db_params, conflict_col)\n",
    "        if len(matches) == 0:\n",
    "            matches[0] = 'done'\n",
    "    json_data = json.dumps(teams_list)\n",
    "    s3.put_object(Bucket = bucket_name, Key = object_key3, Body=json_data)\n",
    "\n",
    "# save current file to s3 bucket\n",
    "data = {'league_id': [league], 'year': [year], 'type': [to_find]}\n",
    "current = pd.DataFrame(data)\n",
    "csv_buffer = io.StringIO()\n",
    "current.to_csv(csv_buffer, index=False)\n",
    "s3.put_object(Bucket = bucket_name, Key = object_key1, Body=csv_buffer.getvalue().encode())\n",
    "\n",
    "# create script for collecting fixtures stats if teams, teams stats and fixtures were collected\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
