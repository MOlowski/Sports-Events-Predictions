{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95fadab1-4bd4-4f0a-95b4-23d37ff40360",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_data(endpoint, params):\n",
    "    \n",
    "    URL = \"https://v3.football.api-sports.io/\"\n",
    "    headers = {\n",
    "\t'x-rapidapi-host': \"v3.football.api-sports.io\",\n",
    "    'x-rapidapi-key': \"fb2140228973d644db847895c454c22b\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        URL+endpoint,\n",
    "        headers = headers,\n",
    "        params = params\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "            \n",
    "        remaining = response.headers.get(\"x-ratelimit-requests-remaining\")\n",
    "        data = response.json()\n",
    "        print(f\"requests before reaching limit {remaining}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}, {response.text}\")\n",
    "\n",
    "    return data, remaining                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb9b678-f7f2-499b-83b4-1469fec1c71c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def encode_data(data_dict, parent_key = '', sep= '_'):\n",
    "    encoded = []\n",
    "    for key, val in data_dict.items():\n",
    "        new_key = f'{parent_key}{sep}{key}' if parent_key else key\n",
    "        if isinstance(val, dict):\n",
    "            encoded.extend(encode_data(val, new_key, sep=sep).items())\n",
    "        elif isinstance(val, list):\n",
    "            if val:\n",
    "                if all(isinstance(i, dict) for i in val):\n",
    "                    for k, v in enumerate(val):\n",
    "                        v_key = f'{new_key}{sep}{k}'\n",
    "                        encoded.extend(encode_data(v, v_key, sep=sep).items())\n",
    "                else:\n",
    "                    encoded.append((new_key, val))\n",
    "            else:\n",
    "                encoded.append((new_key, []))\n",
    "        else:\n",
    "            encoded.append((new_key, val))\n",
    "    return dict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e18f99b-84fc-4651-8497-4465b0b590d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_fix_stats(data, fixture_id):\n",
    "    encoded = []\n",
    "    encoded.append(('fixture_id', fixture_id))\n",
    "    for key, val in data.items():\n",
    "        if key =='team':\n",
    "            encoded.append((key+'_id', val['id']))\n",
    "        else:\n",
    "            for el in val:\n",
    "                encoded.append((el['type'].lower().replace(' ', '_').replace('%', 'percentage'), el['value']))\n",
    "    return dict(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c2906b9-f8b7-4ed3-9588-a9679edb8ffd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def data_to_sql(table_name, df, db_params, conflict_columns):\n",
    "    conn = None\n",
    "    cur = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        #insert data into tables\n",
    "        if len(conflict_columns) == 0:\n",
    "            insert_query = \"\"\"\n",
    "                INSERT INTO {} ({})\n",
    "                VALUES ({})\n",
    "            \"\"\".format(table_name, ','.join(df.columns), ','.join(['%s']*len(df.columns)))\n",
    "        else:\n",
    "            insert_query = \"\"\"\n",
    "                INSERT INTO {} ({})\n",
    "                VALUES ({})\n",
    "                ON CONFLICT ({}) DO NOTHING\n",
    "            \"\"\".format(table_name, ','.join(df.columns), ','.join(['%s']*len(df.columns)), ','.join(conflict_columns))\n",
    "        if len(df) > 0:\n",
    "            last_row = df.iloc[-1]\n",
    "        cur.executemany(insert_query, df.values.tolist())\n",
    "        print(f'table {table_name} updated')\n",
    "        \n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if last_row is not None:\n",
    "            print(f\"Last row loaded before the error occurred: {last_row}\")\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            # Close the cursor and connection\n",
    "            cur.close()\n",
    "        if cur is not None:\n",
    "            conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a93624c-6507-41a7-ba3a-5e8b8c2f23f9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpsycopg2\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import io\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "# get current and european seasons files from bucket\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sport-storage'\n",
    "object_key1 = 'current.csv'\n",
    "object_key2 = 'european_seasons.csv'\n",
    "object_key3 = 'teams.json'\n",
    "\n",
    "response1 = s3.get_object(Bucket = bucket_name, Key = object_key1)\n",
    "response2 = s3.get_object(Bucket = bucket_name, Key = object_key2)\n",
    "response3 = s3.get_object(Bucket = bucket_name, Key = object_key3)\n",
    "\n",
    "current = pd.read_csv(io.BytesIO(response1['Body'].read()))\n",
    "european_seasons = pd.read_csv(io.BytesIO(response2['Body'].read()))\n",
    "\n",
    "# insert parameters to be found\n",
    "league = current['league_id'][0]\n",
    "year = current['year'][0]\n",
    "to_find = current['type'][0]\n",
    "\n",
    "# get current index\n",
    "index = np.where((european_seasons['league_id']==league)&(european_seasons['year']==year))[0][0]\n",
    "\n",
    "remaining = 100\n",
    "total_team_stats_data = []\n",
    "total_teams_data = []\n",
    "done = False\n",
    "\n",
    "# if script ended on collecting team stats get rest of teams\n",
    "if to_find == 'teams/statistics':\n",
    "    teams_list = pickle.loads(response3['Body'].read())\n",
    "else:\n",
    "    teams_list = []\n",
    "\n",
    "# if limit of requests wasnt reached and there is still something to collect enter loop\n",
    "while (done==False)&(remaining > 0):\n",
    "\n",
    "    # get data from API\n",
    "    if to_find == 'teams': \n",
    "        params = {'league': league,\n",
    "                  'season': year\n",
    "                 }\n",
    "    else:\n",
    "        team = teams_list[0]\n",
    "        params = {'league': league,\n",
    "                  'season': year,\n",
    "                  'team': team\n",
    "                 }\n",
    "        \n",
    "    endpoint = to_find\n",
    "    data, remaining_req = get_data(endpoint, params)\n",
    "    \n",
    "    # preprocess data\n",
    "    if endpoint == 'teams':\n",
    "        total_teams_data.extend(encode_data(team) for team in data['response'])\n",
    "    else:\n",
    "        total_team_stats_data.append(encode_data(data['response']))\n",
    "\n",
    "    # if last endpoint is teams now find stats for every team in that season\n",
    "    if endpoint == 'teams':\n",
    "        to_find = 'teams/statistics'\n",
    "        teams_list = [row['team']['id'] for row in data['response']]\n",
    "\n",
    "    # if team stats are collecting drop team that data was already collected for\n",
    "    else:\n",
    "        (endpoint == 'teams/statistics') & (len(teams_list) > 0)\n",
    "        teams_list.pop(0)\n",
    "\n",
    "    # if team stats were collected for every team in current season move to next season\n",
    "        if (endpoint == 'teams/statistics') & (len(teams_list) == 0) & (index < len(european_seasons)-1):\n",
    "            index += 1\n",
    "            league = european_seasons.loc[index]['league_id']\n",
    "            year = european_seasons.loc[index]['year']\n",
    "            to_find = 'teams'\n",
    "\n",
    "    # if data for all seasons were collected quit loop\n",
    "        elif (endpoint == 'teams/statistics') & (len(teams_list) == 0) & (index == len(european_seasons)-1):\n",
    "            done = True\n",
    "        \n",
    "    remaining = int(remaining_req)\n",
    "    print(remaining)\n",
    "    # sleep cause there can be done only 10 requests per minute\n",
    "    time.sleep(7)\n",
    "    \n",
    "    # saving to sql database\n",
    "    # db parameters\n",
    "db_params = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'preds',\n",
    "    'user': 'postgres',\n",
    "    'password': 'pass',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "\n",
    "if len(total_team_stats_data) > 0:\n",
    "    team_stats_df = pd.DataFrame(total_team_stats_data)\n",
    "    team_stats_df = team_stats_df.drop(columns = {\n",
    "        'league_name', \n",
    "        'league_country', \n",
    "        'league_logo', \n",
    "        'league_flag', \n",
    "        'team_name', \n",
    "        'team_logo',\n",
    "        'lineups'})\n",
    "    team_stats_df = team_stats_df.fillna(0)\n",
    "    conflict_col = []\n",
    "    team_stats_df.columns = team_stats_df.columns.str.replace('-','_')\n",
    "    data_to_sql('team_stats', team_stats_df, db_params, conflict_col)\n",
    "                \n",
    "if len(total_teams_data) > 0:\n",
    "    teams_df = pd.DataFrame(total_teams_data)\n",
    "    teams_df = teams_df[['team_id', 'team_name', 'team_country', 'team_logo', 'team_national', 'venue_capacity', 'venue_surface']].rename(columns = {'team_national': 'national'})\n",
    "    conflict_col = ['team_id']\n",
    "    data_to_sql('teams', teams_df, db_params, conflict_col)\n",
    "\n",
    "if not done:\n",
    "    #\n",
    "    json_data = json.dumps(teams_list)\n",
    "    s3.put_object(Bucket = bucket_name, Key = object_key3, Body=json_data)\n",
    "   \n",
    "    data = {'league_id': [league], 'year': [year], 'type': [to_find]}\n",
    "    current = pd.DataFrame(data)\n",
    "    \n",
    "    csv_buffer = io.StringIO()\n",
    "    current.to_csv(csv_buffer, index=False)\n",
    "    s3.put_object(Bucket = bucket_name, Key = object_key1, Body=csv_buffer.getvalue().encode())\n",
    "    \n",
    "    print(f'{len(european_seasons)-index-1} seasons left')\n",
    "else:\n",
    "    print('Teams and their statistics were collected for every season played in Europe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba3c5f2f-c848-4c0a-aad2-78b095f0c3b9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_last_date(db_params, today):\n",
    "    get_data_query = '''\n",
    "    SELECT fixture_id\n",
    "    FROM fixtures \n",
    "    WHERE fixture_status_short != 'FT' and fixture_date < today\n",
    "    '''\n",
    "    res = []\n",
    "    conn = None\n",
    "    cur = None\n",
    "    try:\n",
    "        # Establish the connection\n",
    "        conn = psycopg2.connect(**db_params)\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        cur.execute(get_data_query)\n",
    "        \n",
    "        res = [row[0] for row in cur.fetchall()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        if last_row is not None:\n",
    "            print(f\"Last row loaded before the error occurred: {last_row}\")\n",
    "    finally:\n",
    "        if conn is not None:\n",
    "            # Close the cursor and connection\n",
    "            cur.close()\n",
    "        if cur is not None:\n",
    "            conn.close()\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0248ccd-b443-4d2e-b5bf-a7d051026ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "remaining = 100\n",
    "fixtures_data = []\n",
    "fixture_stats_data = []\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "bucket_name = 'sport-storage'\n",
    "matches_key = 'total_fixs.json'\n",
    "\n",
    "matches_response = s3.get_object(Bucket = bucket_name, Key = matches_key)\n",
    "\n",
    "total_fixs = pickle.loads(matches_response['Body'].read())\n",
    "\n",
    "if len(total_fixs) == 0:\n",
    "    today = date.today()\n",
    "    fixtures_to_find = get_last_date(db_params, today)\n",
    "    total_fixs = fixtures_to_find\n",
    "else:\n",
    "    fixtures_to_find = total_fixs\n",
    "    \n",
    "while (remaining > 0) & (len(fixtures_to_find) > 0):\n",
    "    \n",
    "    ids = ''\n",
    "    sep = '-'\n",
    "    # get fixtures ids to find\n",
    "    if len(fixtures_to_find) > 20:\n",
    "        for i in range(0,20):\n",
    "            ids = f'{ids}{sep}{fixtures_to_find[i]}' if ids else fixtures_to_find[i]\n",
    "        fixtures_to_find = fixtures_to_find[20:]\n",
    "    \n",
    "    elif len(fixtures_to_find) > 0:\n",
    "        for i in fixtures_to_find:\n",
    "            ids = f'{ids}{sep}{i}' if ids else i\n",
    "        fixtures_to_find = []\n",
    "    \n",
    "    #get fixtures\n",
    "    if ids:      \n",
    "        params = {'ids': ids}\n",
    "        \n",
    "        df, remaining = get_data('fixtures', params)\n",
    "        if len(df['response']) > 0:\n",
    "            fixtures_data.extend(encode_data(row) for row in df['response'])\n",
    "        \n",
    "while (remaining > 0) & (len(total_fixs) > 0):\n",
    "\n",
    "    params = {'fixture': total_fixs[0]}\n",
    "    df, remaining = get_data('fixtures/statistics', params)\n",
    "    fixture_stats_data.extend(encode_fix_stats, total_fixs[0])\n",
    "    total_fixs.pop(0)\n",
    "\n",
    "if len(total_fixs) == 0:\n",
    "    print('data is actual')\n",
    "else:\n",
    "    print('data is acutal')\n",
    "\n",
    "\n",
    "\n",
    "if len(fixtures_data) > 0:\n",
    "    fixtures_df = pd.DataFrame(fixtures_data)\n",
    "    fixtures_df = fixtures_df.drop(columns = {\n",
    "        'league_name',\n",
    "        'league_country',\n",
    "        'league_logo',\n",
    "        'league_flag',\n",
    "        'fixture_venue_city',\n",
    "        'fixture_venue_name',\n",
    "        'teams_away_logo',\n",
    "        'teams_away_name',\n",
    "        'teams_home_logo',\n",
    "        'teams_home_name'})\n",
    "    conflict_col = ['fixture_id']\n",
    "    data_to_sql('fixtures', fixtures_df, db_params, conflict_col)\n",
    "    \n",
    "if len(fixture_stats_data) > 0:\n",
    "    fixture_stats_df = pd.DataFrame(fixture_stats_data)\n",
    "    conflict_col = ['fixture_id', 'team_id']\n",
    "    data_to_sql('fixture_statistics', fixture_stats_df, db_params, conflict_col)\n",
    "\n",
    "json_data = json.dump(total_fixs)\n",
    "s3.put_object(Bucket = bucket_name, Key = object_key3, Body=json_data)\n",
    "\n",
    "## update team stats and fixs stats \n",
    "## get fixs to update from list total_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28281429-7bf4-4db1-bd57-5649296be28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "1. get all historical data,\n",
    "2. then find last results date and if season is current get missing results, team stats,\n",
    "\n",
    "3. start collecting data about upcoming matches in next 7 days,\n",
    "4. create predictions model,\n",
    "5. create table for predictions and actual results,\n",
    "6. after that 7 days get real result and save them in fixtures and preds table,\n",
    "7. decide whats better add results to team stats table or collect latest data about team stats (??),\n",
    "8. use preds and real results for models' training,\n",
    "9. create flask app to display preds,\n",
    "10. get upcoming fixtures odds and compare them to preds,\n",
    "11. filter odds worth betting,\n",
    "12. display that bets on flask api\n",
    "13. display statistics for that bets,\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
